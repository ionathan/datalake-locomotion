{"nbformat_minor": 2, "nbformat": 4, "cells": [{"source": ["# Simple ETL for the forceplate data\n", "\n", "Before we are able to transform the data it is necessary to load important packages and libraries."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["import os\n", "from pathlib import Path\n", "from nptdms import TdmsFile as td\n", "\n", "import pixiedust\n", "from pyspark.sql.types import StructType, StructField\n", "from pyspark.sql.types import DoubleType, IntegerType, StringType\n", "from pyspark.sql.functions import input_file_name, lit"], "outputs": [], "metadata": {}}, {"source": ["## Transform native data to csv files\n", "\n", "Data coming from the forceplate sensor is stored in a proprietary format that Spark cannot read directly. \n", "Thus we need to extract the data and store them in an open fromat, as CSV.\n", "\n", "Forceplate data are stored in files with the extention _.tdms_. under a top folder named `files`.  \n", "First we generate a list containing the files in the appropriate folder with the extention _.tdms_.\n", "Secondly, we iterate through the list and transform the native data to _.csv_\n", "format and store them in another folder, under _work_ top folder."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["pathlist = Path(\"files\").glob('**/*.tdms')\n", "for filename in pathlist:\n", "    print(filename)\n", "    directory = os.path.dirname(filename)\n", "    destination = directory + '.csv'\n", "    td(filename).as_dataframe().to_csv('work/forceplate/original/'+os.path.basename(destination))"], "outputs": [], "metadata": {}}, {"source": ["## Read the created .csv file\n", "\n", "To verify if Spark is running in your note, type `spark`.\n", "\n", "Discuss in classroom what the response is."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["spark"], "outputs": [], "metadata": {}}, {"source": ["Had you have a look in the `tdms` files, you notice that the column names are way too long, thus we will define manually the schema into which Spark will load them. \n", "\n", "\n", "In the schema declaratiob below, we define the column names, and the corresponding data types."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["schema = StructType([\n", "    StructField(\"Time\", IntegerType()),\n", "    StructField(\"Channel1\", DoubleType()),\n", "    StructField(\"Channel2\", DoubleType()),\n", "    StructField(\"Channel3\", DoubleType()),\n", "    StructField(\"Channel4\", DoubleType()),\n", "    StructField(\"Channel5\", DoubleType()),\n", "    StructField(\"Channel6\", DoubleType()),\n", "    StructField(\"Channel7\", DoubleType()),\n", "    StructField(\"Channel8\", DoubleType())\n", "])"], "outputs": [], "metadata": {}}, {"source": ["Then, lets read the _.csv_ file, with the schema we set-up earlier."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["channelsDF = spark.read.csv('work/forceplate/original/18936.csv', header=True, schema=schema)"], "outputs": [], "metadata": {}}, {"source": ["Then, lets inspect the dataframe and its schema."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["channelsDF.schema"], "outputs": [], "metadata": {}}, {"source": ["How many columns are there in the dataframe? Run the cell below!"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["len(channelsDF.columns)"], "outputs": [], "metadata": {}}, {"source": ["Use the `count` action to see how many data points are in the dataframe"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["channelsDF.count()"], "outputs": [], "metadata": {}}, {"source": ["To calculate summary statistics from the dataframe, use the function `describe`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["channelsDF.describe().show()"], "outputs": [], "metadata": {}}, {"source": ["If you want to know what Spark is doing behind the hood, the `explain` function prints the physical plan."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["channelsDF.explain()"], "outputs": [], "metadata": {}}, {"source": ["With `select` you can select one or more columns, and chain it with `show` to inspect it."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["channelsDF.select('Channel1').show(2)"], "outputs": [], "metadata": {}}, {"source": ["Keep on exploring the data you have just loaded."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["channelsDF\n", "\n", "channelsDF.show(5)"], "outputs": [], "metadata": {}}, {"source": ["## Calculate  fixed parameters for the Force Plate\n", "\n", "Next step is to calculate the forces on the plate.\n", "![image](http://c-motion.com/v3dwiki/images/b/b1/fp_type6.jpg)\n", "The calculations below are from \n", "[Kistler](https://isbweb.org/software/movanal/vaughan/kistler.pdf) sensor documentation,  \n", "including calculations for:\n", "\n", "- Fx = Medio-lateral force \n", "- Fy = Anterior-posterior force\n", "- Fz = Vertical force\n", "- ax = X-Coordinate of force application point \n", "- ay =Y-Coordinate of force application point\n", "\n", "Constant values for the equations are defined below."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["amp2 = 1.5  # --- amplitude-parameter, van belang voor de eerste grafiek ---\n", "is1 = 2\n", "ixy = 5000  # --- versterker-schaal x/y\n", "is2 = 2\n", "iz = 5000   # --- versterker-schaal z"], "outputs": [], "metadata": {}}, {"source": ["Then the $F_{x}$ is calculated as:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["df = channelsDF.withColumn(\"Fx\",(channelsDF.Channel1+channelsDF.Channel2)*ixy/76.33)"], "outputs": [], "metadata": {}}, {"source": ["Note that we can chain up all these computations as:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["df = channelsDF.withColumn(\"Fx\",(channelsDF.Channel1+channelsDF.Channel2)*ixy/76.33)\\\n", "               .withColumn(\"Fy\",(channelsDF.Channel3+channelsDF.Channel4)*ixy/75.92)\\\n", "               .withColumn(\"Fz\",((channelsDF.Channel5+channelsDF.Channel6+channelsDF.Channel7+channelsDF.Channel8)*iz)/38.65)\\\n", "               .withColumn(\"Fz1\",(channelsDF.Channel5)*iz/38.65)\\\n", "               .withColumn(\"Fz2\",(channelsDF.Channel6)*iz/38.65)\\\n", "               .withColumn(\"Fz3\",(channelsDF.Channel7)*iz/38.65)\\\n", "               .withColumn(\"Fz4\",(channelsDF.Channel8)*iz/38.65)"], "outputs": [], "metadata": {}}, {"source": ["And keep on with some more as follows."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["df = df.withColumn(\"Mx\",(200*(df.Fz1+df.Fz2-df.Fz3-df.Fz4)))\\\n", "                    .withColumn(\"My\",(120*(df.Fz2+df.Fz3-df.Fz1-df.Fz4)))\\\n", "                    #.withColumn(\"Dz\",((channelsDF.Channel5+channelsDF.Channel6+channelsDF.Channel7+channelsDF.Channel8)*iz/38.65[0:50].mean().first()[0]))\\"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["df = df.withColumn(\"Mx1\", (df.Mx + df.Fy*-45.))\\\n", "         .withColumn(\"My1\", (df.My - df.Fx*-45.))\\"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["df = df.withColumn(\"ax\", (-df.My1 / df.Fz))\\\n", "         .withColumn(\"ay\", (df.Mx1 / df.Fz))\\"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["df.show(5)"], "outputs": [], "metadata": {}}, {"source": ["## Explore visually your data \n", "\n", "Now that all forces and quantities have been calculated, we will use pixiedust to visually explore our data. \n", "\n", "We use the command `display(\"yourdf\")` to open the pixiedust GUI in the Jupyter Notebook.\n", "\n", "\n", "Lets start with creating a scatterplot. Select\n", "\n", "- X-axis (i.e. keys) = *Time*, \n", "- Y-axis (i.e. values) = *$F_z$*,\n", "\n", "and set the number of rows to 6000."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["display(df)"], "outputs": [], "metadata": {}}, {"source": ["## Summarize and extract (some) features\n", "\n", "To check if everything is right, print summary statistics"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["df.select(df.Fx,df.Fy).describe().show()\n", "df.select(df.Fz, df.Fz1, df.Fz2, df.Fz3, df.Fz4).describe().show()\n", "df.select(df.Mx,df.Mx1, df.My, df.My1).describe().show()\n", "df.select(df.ax,df.ay).describe().show()"], "outputs": [], "metadata": {}}, {"source": ["To calculate the maximum $F_z$ value, we will use an aggregation function `max`. \n", "Note that an aggregation function is always combined with a `groupBy` function."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["df.select(df.Fz).groupBy().max().first()[0]"], "outputs": [], "metadata": {}}, {"source": ["$F_z$ = m * g, thus m = $F_z$ / g, where g is Newton earth (9.80665)"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["print(\"The turkey weighs approximately\", round(df.select(df.Fz).groupBy().max().first()[0] / 9.80665,2), \"kg\")"], "outputs": [], "metadata": {}}, {"source": ["## Save the file for future use\n", "\n", "\n", "We are finished with preprocessing. Lets store the pre-processed dataset in a\n", "CSV format.\n", "\n", "\n", "First select the columns and then store:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["df_updated = df.select(df.Time, df.Fx, df.Fy, df.Fz, df.Fz1, df.Fz2, df.Fz3, df.Fz4, df.Mx, df.Mx1, df.My, df.My1)\n", "df_updated.write.csv('work/forceplate/updated/18936.csv', header=True, mode='overwrite')"], "outputs": [], "metadata": {}}, {"source": ["Wonderful! Check the newly created file [in the 'work/forceplate/updated'\n", "folder](../tree/work/forceplate/updated).\n", "\n", "Question: What do you observe?\n", "\n", "\n", "## Extra visualizations\n", "\n", "Try to focus your diagram on when the turkey is walking on the forceplate.\n", "\n", "Assuming that all vertical forces below 100 are noize, visualize again $F_z$, as a scatterplot with `Time` on the X-axis and `Fz` on the Y-axis."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["df_updated2 = df_updated.filter(df_updated.Fz>100)\n", "\n", "display(df_updated2)"], "outputs": [], "metadata": {}}, {"source": ["Discuss in pairs what features could be extracted, that can help understand the gait score of turkeys.\n", "\n", "If time allows, play around with different visualizations.\n", "As an example create a scatterplot that shows the coordinates of the center of pressure coordinates (`ax`, `ay`)"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["display(df) #scattered e.g. ax vs. ay"], "outputs": [], "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python with Pixiedust (Spark 2.3)", "name": "pythonwithpixiedustspark23", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "3.6.2", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}