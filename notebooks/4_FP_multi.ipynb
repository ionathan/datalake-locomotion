{"nbformat_minor": 2, "nbformat": 4, "cells": [{"source": ["# Going big by loading several files\n", "\n", "All the steps for pre-processing data from a single forceplate sensor have already been explained in [Tutorial One](3_FP_single.ipynb). \n", "\n", "Here we will try to go \"big\" by batch processing several data files at once!\n", "\n", "In case you have\n", "[Tutorial One](3_FP_single.ipynb) still open, make sure that you have stopped\n", "the Spark engine by running the last command."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["import pixiedust\n", "from pyspark.sql import SparkSession\n", "from pyspark.sql.types import StructType, StructField\n", "from pyspark.sql.types import DoubleType, IntegerType, StringType\n", "from pyspark.sql.functions import *"], "outputs": [], "metadata": {}}, {"source": ["Make sure that Spark works properly."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["spark"], "outputs": [], "metadata": {}}, {"source": ["First we declate the schema of the file, as follows"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["forceplate_schema = StructType([\n", "    StructField(\"Time\", IntegerType()),\n", "    StructField(\"Channel1\", DoubleType()),\n", "    StructField(\"Channel2\", DoubleType()),\n", "    StructField(\"Channel3\", DoubleType()),\n", "    StructField(\"Channel4\", DoubleType()),\n", "    StructField(\"Channel5\", DoubleType()),\n", "    StructField(\"Channel6\", DoubleType()),\n", "    StructField(\"Channel7\", DoubleType()),\n", "    StructField(\"Channel8\", DoubleType())\n", "])"], "outputs": [], "metadata": {}}, {"source": ["## Loading all files\n", "\n", "You may recall from the previous tutorial, that for\n", "loading a single CSV file with Spark, we used a statement like this one:\n", "\n", "`    channelsDF = spark.read.csv('work/forceplate/18936.csv', header=True, schema=forceplate_schema)`\n", "\n", "\n", "Now, we need to load several files that are stored in a folder.  \n", "    \n", " \n", "The only thing we have to change is to replace the filename with the folder location, and\n", "all the CSV files will be read into a single dataframe."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["channelsDFall = spark.read.csv('work/forceplate/original', header=True, schema=forceplate_schema)"], "outputs": [], "metadata": {}}, {"source": ["Check if all files were loaded, for example by printing the length of the file."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["channelsDFall.count()"], "outputs": [], "metadata": {}}, {"source": ["So, the code is working to load all data. Note that this is a massively\n", "parallelizable operation that would have worked the same with really big data\n", "files stored in a remote location. However, this is a task for another tutorial.\n", "You have probably also noticed that the files do not include an identifier of\n", "the turkey they correspond to. This makes our dataframe not useful for the kind\n", "of exercise we try to perform, as we will aim to extract features per turkey. Do\n", "not worry, though, this is a common problem in ETL tasks, thus Spark has some\n", "functionality to solve the problem in a smart way. For each Row in the\n", "dataframe, Spark can recall from which file it was loaded, and we can extract\n", "the Turkey ID information from its path location!\n", "\n", "We can do this by using the\n", "`withColumn` function, together with the `input_file_name` one. \n", "\n", "Check the code\n", "below, where we append a new columnname named *input* that contains the  the\n", "*filename* from which each row has been read. \n", "\n", "The `input_file_name` function\n", "returns the full path of the file from which the row has been read, or an empty\n", "string if not available.\n", "\n", "In our case, this contains the turkey identifier!\n", "Check the code below and inspect the results of the transformations."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["channelsDFall = channelsDFall.withColumn(\"input\", input_file_name())\n", "\n", "channelsDFall.select(\"input\").show(4)"], "outputs": [], "metadata": {}}, {"source": ["### Splitting\n", "Next challenge is to extract the turkey identifier and save is as a separate attribute.\n", "\n", "We need to split the input string (file:///home/jovy...) by `/`, and keep the part with the turkey identifier.  \n", "This can be done by the *split* command.  From the resulting array we pick the 9 element, which contains the\n", "turkey ID."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["channelsDFall = channelsDFall.withColumn('ID', split(channelsDFall['input'], '/')[8])\n", " # Check output #"], "outputs": [], "metadata": {}}, {"source": ["Surely you have noticed that file extension (.csv), is still there. \n", "\n", "To remove this we only keep the first five digits of the splitted element by extracting a substring, using the `substr` command. \n", "\n", "Try the code below and check the output in your ID column."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["channelsDFall = channelsDFall.withColumn('ID', split(channelsDFall['input'], '/')[8].substr(1,5))\n", "channelsDFall\n", "\n", " # Check output #\n", " # i.e. by channelsDFall.show(3)"], "outputs": [], "metadata": {}}, {"source": ["## Extracting features\n", "\n", "Great! You have extracted all necessary information as attributes of your dataframe. \n", "\n", "Next step is to transform them into meaningful attributes for your application!\n", "\n", "Lets assume that we want to calculate two attributes for each turkey: its weight and the time (duration) it was on the forceplate.\n", "\n", "### Feature 1: Turkey weight\n", "\n", "To estimate the weight we will need only the vertical force ($F_z$), which can be calculated as follows (see also previous tutorial)."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": [" # This is a constant of the sensor (versterker-schaal z)\n", "iz = 5000   \n", "\n", "df = channelsDFall.withColumn(\"Fz\",((channelsDFall.Channel5+channelsDFall.Channel6+channelsDFall.Channel7+channelsDFall.Channel8)*iz)/38.65)\n"], "outputs": [], "metadata": {}}, {"source": ["And then inspect what we have achieved so far."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["df.show(5)"], "outputs": [], "metadata": {}}, {"source": ["The first feature is the turkey weight, which for simplicity we estimate as the maximum value of `Fz` divided by Newton constant.\n", "\n", "We need to calculate the max per turkey id, thus we apply the following aggregation grouped by `ID`."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["weights = df.groupBy(df.ID).agg(max(df.Fz).alias('maxFz'))\\\n", "                           .withColumn(\"weight\", round(col('maxFz')/ 9.80665,2))\n", "\n", "weights.show()"], "outputs": [], "metadata": {}}, {"source": ["### Feature 2: Time turkey walked on the forceplate\n", "\n", "\n", "Next we are going to extract the second feature:  the duration (in seconds) of how long the turkey standed on the forceplate.\n", "\n", "Lets assume that all values below 100 is noise, and the turkey is on the plate only for the part that the $Fz >100$.  \n", "\n", "First, lets filter the dataframe to only include the values above 100 in the $F_z$ column."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["df2 = df.filter(df.Fz>100)"], "outputs": [], "metadata": {}}, {"source": ["To estimate the time each turkey spent on the force plate, we first need to select the column *Time*, and calculate the `min` and `max` values,  grouped by the turkey ID. \n", "\n", "In Spark, we can estimate it as:"], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["df2.select(df2.ID, df2.Time).groupBy('ID').min().show()"], "outputs": [], "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["df2.select(df2.ID, df2.Time).groupBy('ID').max().show()"], "outputs": [], "metadata": {}}, {"source": ["Next is to put these two aggregates one next to the other, substract them and multiply by 0.01, as the frequency of the sensor is 100Hz. \n", "\n", "You can do this with the following code."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["df2.groupBy(df.ID).agg(min(df.Time).alias(\"from\"), max(df.Time).alias(\"to\"))\\\n", "                  .withColumn(\"timeOnPlate\",0.01*(col('to')-col('from')))\\\n", "                  .show()\n"], "outputs": [], "metadata": {}}, {"source": ["## Finishing up\n", "\n", "And now lets combine all we have learned to extract both features in one go, and store them in a file, to be ready for followup work."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["df3 = df2.groupBy(df.ID).agg(min(df.Time).alias(\"from\"), max(df.Time).alias(\"to\"), max(df.Fz).alias(\"maxFz\"))\\\n", "                  .withColumn(\"timeOnPlate\",0.01*(col('to')-col('from')))\\\n", "                  .withColumn(\"weight\", round(col('maxFz')/ 9.80665,2))\n", "\n", "df3.show()\n", "\n", "\n", " #save the extracted feauters to file\n", "df3.select('ID','timeOnPlate', 'weight').write.csv(\"fp_features.csv\", header=True, mode='overwrite')"], "outputs": [], "metadata": {}}, {"source": ["Check in you local drive to verify that the file has been stored.\n", "\n", "When you are finished with this notebook, please stop the spark engine before\n", "closing the notebook."], "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": ["spark.stop()"], "outputs": [], "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python with Pixiedust (Spark 2.3)", "name": "pythonwithpixiedustspark23", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "file_extension": ".py", "version": "3.6.2", "pygments_lexer": "ipython3", "codemirror_mode": {"version": 3, "name": "ipython"}}}}